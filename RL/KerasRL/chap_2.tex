%!TEX encoding = UTF-8 Unicode
\documentclass[ % options,
    a4paper,    % papersize
%    cjk,       % for cjk-ko
%    usedotemph,% for cjk-ko's \dotemph
    amsmath,    % load amsmath.sty to typeset math materials
    itemph,     % to disable gremph default (xe/lua)
%    footnote,  % korean style footnote
%    chapter,   % to use \chapter
]{oblivoir}     % xoblivoir and oblivoir are identical.

\ifPDFTeX       % latex, pdflatex
%    \usepackage{newtxtext}    % Latin fonts
\else\ifLuaOrXeTeX   % xelatex or lualatex
%  \setmainfont{TeX Gyre Termes}   %% Latin fonts
%	\setkomainfont(Noto Serif CJK KR)(* Bold)(* Medium)
%	\setkosansfont(Noto Sans CJK KR)(* Bold)(* Medium)
\fi\fi

% packages
\usepackage{kotex-logo}
\usepackage[utf]{kotex}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=10mm,
 }
 
%% font packages and setup
\usepackage{fontspec}
\setmainfont{UnDotum}
\setsansfont{UnDotum}
\setmonofont{UnTaza}
\usepackage{dhucs-interword}
\interhword[.6]{.475}{.1}{.1}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

% operator
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\E}{\mathbb{E}}

\begin{document}

\title{강화학습}
\author{wisemountain}
\date{\today}

\maketitle

\newpage

\tableofcontents

\newpage

\section{그리드 월드와 다이나믹 프로그래밍}

\subsection{벨만 기대방정식과 최적 방정식}

수식 2.31 벨만 기대 방정식:
$$
v_\pi(s) = \E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = s] 
$$

수식 2.32 계산 가능한 벨만 기대 방정식: 
$$
v_\pi(s) = \sum_{a\in A} \pi(a|s) (R_{t+1} + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi(s'))
$$

수식 2.36 최적의 가치함수: 
$$
v_*(s) = \max_\pi [v_\pi(s)]
$$ 

수식 2.37 최적의 큐함수 (상태행동 가치함수): 
$$
q_*(s, a)= \max_\pi[q_\pi(s,a)]
$$

수식 2.38 최적 정책 :
$$
\pi_*(s, a) = 
	\begin{cases}
		1 \text{ if } a = \underset{a \in A}\argmax q_*(s, a) \\
		0 \text{ otherwise }
	\end{cases}
$$

수식 2.39 큐 함수에 최대를 선택하는 최적 가치함수 :
$$
v_*(s) = \max_a [ q_*(s, a) | S_t = a, A_t = a]
$$

수식 2.40 벨만 최적 방정식 :
$$
v_*(s) = \max_a \E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]
$$

수식 2.41 큐함수에 대한 벨만 최적 방정식 :
$$
q_*(s, a) = \E[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a]
$$

단단한 강화학습에서 식 2.41을 계산 가능한 형태로 푼 수식은 다음과 같다. 
$$
q_*(s, a) = \sum_{s', r} p(s', r|s, a)[r + \gamma \max_{a'} q_*(s', a')]
$$

여기서 $p(s', r|s, a)$는 $P_{ss'}^{a}$에 r 값 분기가 추가된 것으로 단단한 강화학습에서는 
동일한 행동과 동일한 다음 상태에 대해 서로 다른 보상이 주어지는 분기가 있다고 본다. 

\subsection{다이나믹 프로그래밍} 

큰 문제를 작은 문제로 나누고 작은 문제들을 계산하여 기억한 후
그 값들을 사용하여 큰 문제를 단계적으로 계산하는 최적화 
알고리즘이다. 벨만 방정식(기대, 최적) 모두 재귀적인 형태로 되어 있고 
이전 값을 사용하여 갱신해서 최적 값을 얻는다. 

정책은 무작위 정책에서 상태 가치 함수와 행동 가치 함수를 사용하여 
점차 최적의 가치를 돌려주는 정책으로 만들어 간다. 


\end{document}
